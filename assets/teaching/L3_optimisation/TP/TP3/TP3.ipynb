{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p hidden>Here are some Latex definitions</p> \n",
    "\n",
    "$\\newcommand{\\R}{\\mathbb{R}}$\n",
    "$\\newcommand{\\RR}{\\mathbb{R}}$\n",
    "$\\newcommand{\\N}{\\mathcal{N}}$\n",
    "$\\newcommand{\\E}{\\mathbb{E}}$\n",
    "$\\newcommand{\\Mm}{\\mathcal{M}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TP 3 : Minimisation d'une fonction quadratique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy.linalg import norm, svd, solve\n",
    "from numpy.random import randn\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Permet a une cellule d'avoir plus d'un display en sortie\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "# Seed for np.random\n",
    "np.random.seed(seed=78)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I. Algorithme du gradient pour une fonction quadratique\n",
    "\n",
    "Etant donné $A$ une matrice symmétrique de $\\R^{N\\times N}$, et $b\\in \\R^N$, on cherche obtenir un vecteur minimisant la fonction\n",
    "\\begin{equation}\n",
    "f(x) = \\frac{1}{2}\\Vert Ax - b \\Vert^2.\n",
    "\\tag{1}\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "On rappelle l'**Algorithme du Gradient à Pas Fixe** pour une fonction de gradient $L$-Lipschitzien:\n",
    "\n",
    "| | | |\n",
    "|-|-|-|\n",
    "|(GPF)| On choisit $x_0$ un vecteur de $\\R^N$ et $\\rho \\in ]0,2/L[$ un pas fixe. |\n",
    "|  | Pour $k\\geq 0$ : $x_{k+1}  = x_k  - \\rho \\nabla f(x_k)$. |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Voici quelques commandes python dont vous pourrez avoir besoin:\n",
    "\n",
    "| | |\n",
    "|-|-|\n",
    "|`randn(m,n)`| Génère une matrice aléatoire dans $\\Mm_{m,n}(\\RR)$ |\n",
    "| `np.zeros((m,n))` | Génère une matrice nulle dans $\\Mm_{m,n}(\\RR)$ |\n",
    "| `np.ones((m,n))` | Génère une matrice remplie de $1$ dans $\\Mm_{m,n}(\\RR)$ |\n",
    "| `A.T` | Transposée de la matrice `A` |\n",
    "| `A@B` | Produit entre deux matrices ou matrice/vecteur |\n",
    "| `norm(A,2)` | Plus grande valeur singulière de `A` |\n",
    "| `norm(A,-2)` | Plus petite valeur singulière de `A` |\n",
    "| `norm(x)` | Norme Euclidienne du vecteur `x` |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1)** Définir une matrice $A \\in \\Mm_{50,100}(\\RR)$, et un vecteur $y \\in \\RR^{50}$, tous deux alétoires. On utilisera la fonction `randn(m,n)` qui génère une matrice dans $\\Mm_{m,n}(\\RR)$ dont les coefficients suivent la loi normale centrée réduite. \n",
    "\n",
    "Ici, et durant toute la suite du TP, les vecteurs devront être considérés comme des matrices sous forme de colonne colonne."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2)** Rappel du cours/TD: Le gradient de la fonction $f$ en $x$ vaut $\\nabla f(x) = A^*(Ax-b)$ et $\\nabla^2 f(x) \\equiv A^*A$. Calculer $L$, la constante de Lipschitz du gradient de $f$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.1)** Définir une fonction `algo_gradient` qui:\n",
    "- prend en arguments une matrice `A`, un vecteur `b`, un point initial `x0`, un pas `rho`, et un nombre d'itérations maximal `itermax`\n",
    "- applique l'algorithme du gradient à pas constant à $f$, en partant de `x0`, pendant `itermax` itérations\n",
    "- renvoie le dernier itéré de la suite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def algo_gradient(A, b, x0, rho, itermax):\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.2)** Vérifier que votre fonction marche bien, en la testant avec $\\rho=1/L$, `nmax`$=10^3$ et un point initial de votre choix dans $\\RR^{100}$. Vous vérifierez également que la solution `x` ainsi obtenue satisfait $\\nabla f(x) \\simeq 0$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.3)** Vu que l'algorithme a l'air de marcher très bien, on n'a peut être pas besoin de le faire tourner pour toutes les `itermax` itérations. Modifier `algo_gradient` afin que:\n",
    "- il prenne en nouvel argument un niveau de tolérance `tol`\n",
    "- il s'arrête dès lors que $\\Vert \\nabla f(x) \\Vert <$ `tol`\n",
    "- il renvoie en plus le nombre d'itérations qui ont été effectuées.\n",
    "\n",
    "Pour cela, vous pourrez utiliser l'instruction `break` qui permet d'arrêter une boucle (voir le petit exemple ci-dessous pour comprendre comment `break` fonctionne en python)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exemple de focntionnement pour BREAK\n",
    "for boucle_exter in [1,2]:\n",
    "    print(\"Boucle externe numéro \"+str(boucle_exter))\n",
    "    for lettre in \"abcde\":\n",
    "        print(lettre)\n",
    "        if lettre == \"c\":\n",
    "            break\n",
    "    print(\"La boucle interne s'est arrêtée à : \"+lettre)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nouvelle version de algo_gradient\n",
    "def algo_gradient(A, b, x0, rho, itermax, tol):\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    return x, k"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.4)** Tester la nouvelle version de l'algorithme avec les mêmes paramètres qu'à la question **3.2)**, et une tolérance de $10^{-10}$. De combien d'itérations a-t-on besoin?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4)** On souhaite maintenant que l'algorithme aille le plus vite possible. Pour cela, on va se servir du cours, où on a vu que l'algorithme du gradient à pas fixe converge le plus vite lorsque $\\rho = \\frac{2}{\\mu+L}$, où $\\mu$ est la plus petite valeur singlulière de $A^*A$.\n",
    "\n",
    "**4.1)** Calculer `mu` de deux façons différentes: $\\sigma_{min}(A^*A)$ et $\\sigma_{min}(A)^2$. Que constatez-vous? Lequel donne un résultat satisfaisant? A votre avis, pourquoi?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4.2)** Utiliser l'algorithme comme à la question **3.4)** en remplaçant le pas $1/L$ par $\\frac{2}{\\mu+L}$. Observer le nombre d'itérations qu'il faut maintenant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4.3)** Est-ce que ce problème est bien conditionné?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5)** Le cours prédit que \n",
    "\\begin{eqnarray}\n",
    "f(x_{k+1}) - \\inf f & \\leq & \\theta^{2k} \\  (f(x_k) - \\inf f),\n",
    "\\end{eqnarray}\n",
    "\n",
    "où $\\theta = (L - \\mu)/L$ si $\\rho = 1/L$, et $\\theta = (L- \\mu)/(L+\\mu)$ si $\\rho = 2/(L + \\mu)$.\n",
    "On va vérifier sur notre exemple si ces vitesses sont respectées.\n",
    "\n",
    "**5.1)** Modifier la fonction `algo_gradient` afin que:\n",
    "- il renvoie en plus une `liste` qui contient tous les itérés de la suite."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nouvelle version de algo_gradient\n",
    "def algo_gradient(A, b, x0, rho, itermax, tol):\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    return x, k, suite"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5.2)** Faire tourner l'algorithme du gradient avec $\\rho  =1/L$, et construire deux listes :\n",
    "- `val` qui contient les valeurs $f(x_k) - \\inf f$\n",
    "- `val_theorie` qui contient la borne supérieure $\\theta^{2k} (f(x_0) - \\inf f)$.\n",
    "\n",
    "On cherchera un moyen rapide de calculer $\\inf f$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5.3)** Vérifier si la décroissance des valeurs respecte la thérorie : on tracera deux courbes en échelle logarithmique (l'échelle logarithmique s'active avec `plt.yscale('log')`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5.4)** Reprendre ces deux dernières questions pour $\\rho = 2/(L+\\mu)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# II. Application au traitement d'images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# N'exécuter cette cellule qu'une seule fois (prend un peu de temps)\n",
    "import sys\n",
    "!{sys.executable} -m pip install git+https://github.com/Guillaume-Garrigos/invprob.git\n",
    "# python -m pip install git+https://github.com/Guillaume-Garrigos/invprob.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Depuis Moodle, télécharger le fichier `photo.npy`, et le placer dans le même dossier que celui contenant ce notebook. Vous pourrez ensuite charger ce fichier comme une image dans python via la commande `np.load`, qui l'importe comme une matrice (dont les coefficients correspondent aux pixels) ; et l'afficher avec la commande `imshow`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from invprob.signal import imread, imshow, gaussian_kernel, dct2, idct2\n",
    "from invprob.general import dotp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = np.load('photo.npy')\n",
    "imshow(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le but de cette section va être d'essayer de retirer l'effet de flou sur cette image, en la rendant plus nette.\n",
    "Pour cela nous allons simplement *modéliser* notre problème comme un problème inverse linéaire:\n",
    "- il existe une certaine image originale, notons-la $x$, que l'on voudrait récupérer.\n",
    "- cette image a subi un \"flou\", ce qui a abouti à l'image que l'on a téléchargé. Appelons cette image $b$.\n",
    "- l'opération de floutage est en fait une opération *linéaire* (on l'admet pour l'instant), donc on peut dire que $b=Ax$, où $A$ est l'application linéaire de floutage.\n",
    "- Pour retrouver $x$ à partir de $b$, il nous \"suffit\" donc de résoudre le système $Ax=b$, ce qui revient à minimiser $\\Vert Ax-b \\Vert^2$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1)** Commençons par nous intéresser à cette application linéaire de floutage $A$, définie ci-dessous non pas comme une matrice mais via la fonction `flou()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel_size = 3\n",
    "kernel_std = 1\n",
    "im_shape = (128,128)\n",
    "kernel_fourier = gaussian_kernel(kernel_size, kernel_std, im_shape)\n",
    "\n",
    "def flou(x): # La fonction qui permet d'évaluer A@x\n",
    "    return np.real(idct2(dct2(x) * kernel_fourier))\n",
    "\n",
    "L = np.max(np.abs(kernel_fourier)) # Ceci est la constante de Lipschitz du gradient de la fonction quadratique"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.1** Soit $b$ l'image précédemment importée. Calculer $Ab$, et afficher le résultat avec `imshow`. Faire de même avec $A^4b$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.2)** Comment se convaincre que cette application `flou` est linéaire? Très simple: générer deux matrices aléatoires $X$ et $Y$ dans $\\Mm_{128,128}(\\RR)$, et vérifier que $A(X+Y)=AX + AY$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.3)** Non seulement `flou` est linéaire, mais elle est en plus symétrique, c'est-à-dire que $A^*=A$. Pour s'en convaincre, il suffit de prendre deux matrices aléatoires $X$ et $Y$ et de vérifier que $\\langle AX,Y\\rangle = \\langle X,AY \\rangle$. On pourra utiliser la fonction `dotp(.,.)` qui calcule le produit scalaire entre deux vecteurs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.1)** Reprendre le code de la question **I.3.3)** pour coder une fonction `defloutage`, qui:\n",
    "- prend en entrée une image floutée `b`, un nombre maximal d'itérations `itermax`, un paramètre de tolérance `tol`\n",
    "- applique l'algorithme du gradient à pas constant à $\\Vert Ax-b \\Vert^2/2$, où $A$ est l'application de flou, en initialisant l'algorithme à zéro, et en prenant comme pas $\\rho = 1.9/L$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def defloutage(b, itermax, tol):\n",
    "    \n",
    "    \n",
    "    return x, k"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.2)** Tester cette fonction avec `itermax=10000` et une tolérance de $10^{-4}$. Comparer le résultat avec la photo floutée originale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = plt.figure(dpi=100)\n",
    "_ = plt.subplot(1,2,1)\n",
    "imshow(b) # l'image floutée\n",
    "_ = plt.subplot(1,2,2)\n",
    "imshow(x) # l'image obtenue par l'algorithme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
