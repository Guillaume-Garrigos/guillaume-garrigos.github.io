---
image:
---

In the last months I submitted  a few papers discussing the consitency of low-complexity models (sparsity, low-rank, ...), their implications from an optimisation perspective, and applications to learning problems: 
	
<b>Thresholding gradient methods in Hilbert spaces: support identification and linear convergence</b>, with <a href="http://web.mit.edu/lrosasco/www/">L. Rosasco</a> and <a href="http://lcsl.mit.edu/data/silviavilla/Home.html">S. Villa</a>,  Preprint on <a href="https://arxiv.org/abs/1712.00357">arXiv:1712.00357</a>. 

<b>Sparse Multiple Kernel Learning: Support Identification via Mirror Stratifiability</b>, with <a href="http://web.mit.edu/lrosasco/www/">L. Rosasco</a> and <a href="http://lcsl.mit.edu/data/silviavilla/Home.html">S. Villa</a>, Preprint on <a href="https://arxiv.org/abs/arXiv:1803.00783">arXiv:1803.00783</a>.

<b>Model Consistency for Learning with Mirror-Stratifiable Regularizers</b>, with <a href="https://fadili.users.greyc.fr/">J. Fadili</a>, <a href="https://ljk.imag.fr/membres/Jerome.Malick/">J. Malick</a> and <a href="http://www.gpeyre.com/">G. Peyr√©</a> , Preprint on <a href="https://arxiv.org/abs/arXiv:1803.08381">arXiv:1803.08381</a>. 
